{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/cc-ats/mlp_class/blob/main/Lesson3_BP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behler-Parrinello  Symmetry Functions\n",
    "\n",
    "In this tutorial, we will create Behler-Parrinello and ANI neural networks using atom-centered symmetry functions. The symmetry functions will allow us to ensure that our observables (such as energy) are invariant to translations and rotations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data and Libraries\n",
    "\n",
    "We will first import data and libraries needed to train our machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data From GitHub (mlp_class) and Installing PyTorch Lightning\n",
    "\n",
    "\n",
    " Files from GitHub:\n",
    " - Coordinates and atom types:\n",
    "- **qm_coord.npy** (1800, 14, 3)\n",
    "- **qm_elem.txt** ([6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    " - PM3 energies and gradients:\n",
    "- **energy_sqm.npy** (1800,)\n",
    "- **qm_grad_sqm.npy** (1800, 14, 3)\n",
    "\n",
    " - B3LYP/6-31+G* energies and gradients:\n",
    "- **energy.npy**  (1800,)\n",
    "- **qm_grad.npy** (1800, 14, 3)\n",
    "\n",
    "These files will provide us with semi-empirical (PM3) and QM (B3LYP/6-31+G*) data to train our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch-lightning > /dev/null   #installing PyTorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "We will import the following libraries:\n",
    "\n",
    "- math, typing (Sequence, Tuple)\n",
    "- Torch (nn, nn.functional, Tensor. TensorDataset, DataLoader, random_split)\n",
    "- PyTorch Lightning (loggers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetry Functions\n",
    "\n",
    "So far, we have used cartesian coordinates and energies as inputs for our machine learning models. However, cartesian coordinates are not ideal for use with neural networks because translating or rotating the molecule can change the neural network output. In the case of chemistry this would imply that translating a molecule in space changes the molecule's energy, which is incorrect.\n",
    "\n",
    "Instead, we can describe our cartesian coordinates with symmetry functions and use our newly transformed coordinates as inputs, which will produce outputs that are invariant to translations and rotations. \n",
    "\n",
    "From Behler and Parrinello's 2007 paper (***cite), several symmetry functions and a cutoff function were introduced:   \n",
    "<br />\n",
    "The cutoff function is defined as:  \n",
    "<br />  \n",
    "\\begin{align}\n",
    "        f_c(R_{ij}) = \\left\\{\n",
    "        \\begin{array}{cl}\n",
    "        0.5\\times\\left[\\cos\\left(\\frac{\\pi R_{ij}}{R_c}\\right)\\right] & \\textrm{for } R_{ij} \\le R_c \\\\\n",
    "        0 & \\textrm{for } R_{ij} \\gt R_c\n",
    "        \\end{array}\n",
    "        \\right.\n",
    "    \\end{align}\n",
    "\n",
    "Where $R_{ij}$ is the distance between atoms and $R_c$ is the cutoff distance.  \n",
    "<br /> \n",
    "\n",
    "The radial symmetry function can be defined as: <br />  \n",
    "\\begin{align}\n",
    "\\begin{array}{c}\n",
    "G_i^1 = \\sum_{j \\neq i}^{all} e^{-\\eta(R_{ij}-R_s)^2}f_c(R_{ij}) \n",
    "\\end{array}\n",
    "\\end{align}\n",
    "\n",
    "<br />\n",
    "Where $\\eta$ changes the width of the Gaussian distribution and $R_s$ shifts the center of the gaussian peak.  \n",
    "<br /> \n",
    "\n",
    "The angular symmetry function can be defined as: \n",
    "<br />  \n",
    "\\begin{align}\n",
    "\\begin{array}{c}\n",
    "G_i^2 = 2^{1-\\zeta}\\sum_{j,k\\neq i}^{all} (1+\\lambda \\cos\\theta_{ijk})^\\zeta \\times e^{-\\eta(R_{ij}^2+R_{ik}^2+R_{jk}^2)}f_c(R_{ij}) f_c(R_{ik}) f_c(R_{jk}) \n",
    "\\end{array}\n",
    "\\end{align}\n",
    "<br />\n",
    "\n",
    "The angular terms are created for each set of 3 atoms as $\\theta_{ijk} = \\frac{\\mathbf{R_{ij}}\\cdot\\mathbf{R_{ik}}}{R_{ij}R_{ik}}$, where $\\mathbf{R_{ij}} = \\mathbf{R_{i}} - \\mathbf{R_{j}}$ and gives the distance vector between atoms i and j. The $\\lambda$ parameter is set to +1,-1 to allow exploration of the angular environment at the +1 and -1 positions. The parameter $\\eta$ changes the width of the gaussian distribution and $\\zeta$ changes the width of the gaussians in the angular environment.   \n",
    "<br /> \n",
    "\n",
    "In 2017, Isayev and Roitberg developed the ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies), also referred to as ANI (***cite). ANI uses a modified angular symmetry function defined as:  <br />   \n",
    "\\begin{align}\n",
    "\\begin{array}{c}\n",
    "G_i^2 = 2^{1-\\zeta}\\sum_{j,k\\neq i}^{all} (1+\\cos(\\theta_{ijk}-\\theta_s))^\\zeta \\times e^{-\\eta\\left(\\frac{R_{ij}+R_{ik}}{2}-R_{s}\\right)^2}f_c(R_{ij}) f_c(R_{ik})\n",
    "\\end{array}\n",
    "\\end{align}    \n",
    "<br />\n",
    "\n",
    "Here, $\\theta_s$ is a parameter that allows for shifts in the angular environment and $R_s$ allows for calculating the angular environment in radial shells by shifting the center of the gaussians. The result is probing the angular environment at different radial lengths and angles instead of only at $\\lambda$ values of +1,-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Symmetry Functions\n",
    "\n",
    "Now we will define the radial symmetry functions:\n",
    "\n",
    "- **pairwise_vector:** Distance function for calculating distances between each atom.\n",
    "\n",
    "- **symmetry_function_g2:** BP radial symmetry function.\n",
    "\n",
    "- **symmetry_function_g3:** BP angular symmetry function.\n",
    "\n",
    "- **symmetry_function_g3ani:** ANI angular symmetry function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_vector(coords: Tensor) -> Tensor:\n",
    "    num_batches, num_channels, _ = coords.size()\n",
    "    rij = coords[:, :, None] - coords[:, None]\n",
    "    mask = ~torch.eye(num_channels, dtype=torch.bool, device=coords.device) # remove self-interaction\n",
    "    rij = torch.masked_select(rij, mask.unsqueeze(2)).view(num_batches, num_channels, num_channels - 1, 3)\n",
    "    return rij\n",
    "\n",
    "\n",
    "def symmetry_function_g2(rij: Tensor, Rcr: float, EtaR: Tensor, ShfR: Tensor) -> Tensor:\n",
    "    dij = torch.norm(rij, dim=3)\n",
    "    fij = (torch.cos(dij / Rcr * math.pi) + 1) * 0.5\n",
    "    g2 = torch.sum(torch.exp(-EtaR.unsqueeze(dim=1) * (dij.unsqueeze(dim=-1) - ShfR.unsqueeze(dim=1))**2) * fij.unsqueeze(dim=-1), dim=2)\n",
    "    return g2\n",
    "\n",
    "\n",
    "def symmetry_function_g3(rij: Tensor, Rca: float, Zeta: Tensor, EtaA: Tensor) -> Tensor:\n",
    "    c = torch.combinations(torch.arange(rij.size(2)), r=2)\n",
    "    rij = rij[:, :, c]\n",
    "    r12 = rij[:, :, :, 0]\n",
    "    r13 = rij[:, :, :, 1]\n",
    "    r23 = r12 - r13\n",
    "    d12 = torch.norm(r12, dim=3)\n",
    "    d13 = torch.norm(r13, dim=3)\n",
    "    d23 = torch.norm(r23, dim=3)\n",
    "    f12 = (torch.cos(d12 / Rca * math.pi) + 1) * 0.5\n",
    "    f13 = (torch.cos(d13 / Rca * math.pi) + 1) * 0.5\n",
    "    f23 = (torch.cos(d23 / Rca * math.pi) + 1) * 0.5\n",
    "    cosine = torch.einsum('ijkl,ijkl->ijk', r12, r13) / (d12 * d13)\n",
    "\n",
    "    g3 = torch.sum(2**(1 - Zeta.unsqueeze(dim=1)) * (1 + cosine.unsqueeze(dim=-1))**Zeta.unsqueeze(dim=1) * torch.exp(-EtaA.unsqueeze(dim=1) * (d12**2 + d13**2 + d23**2).unsqueeze(dim=-1)) * (f12 * f13 * f23).unsqueeze(dim=-1), dim=2)\n",
    "    return g3\n",
    "\n",
    "\n",
    "def symmetry_function_g3ani(rij: Tensor, Rca: float, Zeta: Tensor, ShfZ: Tensor, EtaA: Tensor, ShfA: Tensor) -> Tensor:\n",
    "    c = torch.combinations(torch.arange(rij.size(2)), r=2)\n",
    "    rij = rij[:, :, c]\n",
    "    r12 = rij[:, :, :, 0]\n",
    "    r13 = rij[:, :, :, 1]\n",
    "    r23 = r12 - r13\n",
    "    d12 = torch.norm(r12, dim=3)\n",
    "    d13 = torch.norm(r13, dim=3)\n",
    "    f12 = (torch.cos(d12 / Rca * math.pi) + 1) * 0.5\n",
    "    f13 = (torch.cos(d13 / Rca * math.pi) + 1) * 0.5\n",
    "    cosine = torch.einsum('ijkl,ijkl->ijk', r12, r13) / (d12 * d13)\n",
    "    cosine = torch.cos(torch.acos(cosine).unsqueeze(dim=-1) - ShfA.unsqueeze(dim=1))\n",
    "\n",
    "    g3 = torch.sum(2**(1 - Zeta.unsqueeze(dim=1)) * (1 + cosine)**Zeta.unsqueeze(dim=1) * torch.exp(-EtaA.unsqueeze(dim=1) * (0.5 * (d12 + d13).unsqueeze(dim=-1) - ShfZ.unsqueeze(dim=1))**2) * (f12 * f13).unsqueeze(dim=-1), dim=2)\n",
    "    return g3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network and Feature Extraction Classes\n",
    "\n",
    "We will now define feature extraction classes which will be used in our Behler-Parrinello and ANI neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Sequential and Dense Classes\n",
    "\n",
    "The Sequential class contains the function for the feed-forward aspect of our neural network.\n",
    "The Dense class contains functions to describe a densely connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Sequential(nn.Sequential):\n",
    "    def forward(self, input: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        for module in self:\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, num_channels: int, in_features: int, out_features: int, bias: bool = True, activation: bool = False, residual: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_channels, out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_channels, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.activation = activation\n",
    "        self.residual = residual\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        for w in self.weight:\n",
    "            nn.init.kaiming_uniform_(w, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            for b, w in zip(self.bias, self.weight):\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                nn.init.uniform_(b, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        x, channels = input\n",
    "        weight: Tensor = self.weight[channels]\n",
    "        output: Tensor = torch.bmm(x.transpose(0, 1), weight.transpose(1, 2)).transpose(0, 1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            bias = self.bias[channels]\n",
    "            output = output + bias\n",
    "\n",
    "        if self.activation:\n",
    "            output = torch.tanh(output)\n",
    "\n",
    "        if self.residual:\n",
    "            if output.shape[2] == x.shape[2]:\n",
    "                output = output + x\n",
    "            elif output.shape[2] == x.shape[2] * 2:\n",
    "                output = output + torch.cat([x, x], dim=2)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "        return output, channels\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'num_channels={}, in_features={}, out_features={}, bias={}, activation={}, residual={}'.format(\n",
    "            self.num_channels, self.in_features, self.out_features, self.bias is not None, self.activation, self.residual\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Feature Class\n",
    "\n",
    "Now we will create the feature class, which takes input information and uses functions to extract characteristics unique to the called input features. Here we will be using the previously defined symmetry functions for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self, Rcr: float, EtaR: Tensor, ShfR: Tensor, Rca: float, Zeta: Tensor, EtaA: Tensor) -> None:\n",
    "        super().__init__()\n",
    "        assert len(EtaR) == len(ShfR)\n",
    "        assert len(Zeta) == len(EtaA)\n",
    "        self.Rcr = Rcr\n",
    "        self.Rca = Rca\n",
    "        self.EtaR = torch.Tensor(EtaR)\n",
    "        self.ShfR = torch.Tensor(ShfR)\n",
    "        self.Zeta = torch.Tensor(Zeta)\n",
    "        self.EtaA = torch.Tensor(EtaA)\n",
    "\n",
    "    def forward(self, coords: Tensor, atom_types: Tensor) -> Tensor:\n",
    "        num_batches, num_channels, _ = coords.size()\n",
    "        rij = pairwise_vector(coords)\n",
    "        EtaR = self.EtaR.to(device=coords.device)[atom_types]\n",
    "        ShfR = self.ShfR.to(device=coords.device)[atom_types]\n",
    "        Zeta = self.Zeta.to(device=coords.device)[atom_types]\n",
    "        EtaA = self.EtaA.to(device=coords.device)[atom_types]\n",
    "        g2 = symmetry_function_g2(rij, self.Rcr, EtaR, ShfR)\n",
    "        g3 = symmetry_function_g3(rij, self.Rca, Zeta, EtaA)\n",
    "\n",
    "        return torch.concat((g2, g3), dim=2)\n",
    "\n",
    "    @property\n",
    "    def output_length(self) -> int:\n",
    "        return len(self.EtaR[0]) + len(self.EtaA[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Feature Class for ANI\n",
    "\n",
    "We will create a similar feature class for ANI that uses the ANI angle symmetry function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureANI(nn.Module):\n",
    "    def __init__(self, Rcr: float, EtaR: Tensor, ShfR: Tensor, Rca: float, Zeta: Tensor, ShfZ: Tensor, EtaA: Tensor, ShfA: Tensor) -> None:\n",
    "        super().__init__()\n",
    "        assert len(EtaR) == len(ShfR)\n",
    "        assert len(Zeta) == len(ShfZ) == len(EtaA) == len(ShfA)\n",
    "        self.Rcr = Rcr\n",
    "        self.Rca = Rca\n",
    "        self.EtaR = torch.Tensor(EtaR)\n",
    "        self.ShfR = torch.Tensor(ShfR)\n",
    "        self.Zeta = torch.Tensor(Zeta)\n",
    "        self.ShfZ = torch.Tensor(ShfZ)\n",
    "        self.EtaA = torch.Tensor(EtaA)\n",
    "        self.ShfA = torch.Tensor(ShfA)\n",
    "\n",
    "    def forward(self, coords: Tensor, atom_types: Tensor) -> Tensor:\n",
    "        num_batches, num_channels, _ = coords.size()\n",
    "        rij = pairwise_vector(coords)\n",
    "        EtaR = self.EtaR.to(device=coords.device)[atom_types]\n",
    "        ShfR = self.ShfR.to(device=coords.device)[atom_types]\n",
    "        Zeta = self.Zeta.to(device=coords.device)[atom_types]\n",
    "        ShfZ = self.ShfZ.to(device=coords.device)[atom_types]\n",
    "        EtaA = self.EtaA.to(device=coords.device)[atom_types]\n",
    "        ShfA = self.ShfA.to(device=coords.device)[atom_types]\n",
    "        g2 = symmetry_function_g2(rij, self.Rcr, EtaR, ShfR)\n",
    "        g3 = symmetry_function_g3ani(rij, self.Rca, Zeta, ShfZ, EtaA, ShfA)\n",
    "\n",
    "        return torch.concat((g2, g3), dim=2)\n",
    "\n",
    "    @property\n",
    "    def output_length(self) -> int:\n",
    "        return len(self.EtaR[0]) + len(self.EtaA[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Fitting Class\n",
    "\n",
    "Now we create the fitting class, which will use the dense neural network for fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitting(nn.Module):\n",
    "    def __init__(self, n_types: int, in_features: int, neuron: Sequence[int] = [240, 240, 240]) -> None:\n",
    "        super().__init__()\n",
    "        layers = [Dense(n_types, in_features, neuron[0], activation=True)] # using the Dense class defined in 3.3 \n",
    "        for i in range(len(neuron)-1):\n",
    "            layers.append(Dense(n_types, neuron[i], neuron[i+1], activation=True, residual=True))  # iterating through the neurons\n",
    "        layers.append(Dense(n_types, neuron[-1], 1))\n",
    "        self.fitting_net = Sequential(*layers)\n",
    "\n",
    "    def forward(self, input : Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        output, _ = self.fitting_net(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Behler-Parrinello Neural Network (BPNN) Class\n",
    "\n",
    "Finally, we will now create the BPNN class that can be used for training with the symmetry functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPNN(pl.LightningModule):\n",
    "    def __init__(self, descriptor: nn.Module, fitting_net: nn.Module, learning_rate=5e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.descriptor = descriptor\n",
    "        self.fitting_net = fitting_net\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, coords: torch.Tensor, atom_types: torch.Tensor):\n",
    "        coords.requires_grad_()\n",
    "        descriptors = self.descriptor(coords, atom_types)\n",
    "        atomic_energies = self.fitting_net((descriptors, atom_types))\n",
    "        energy = torch.unbind(torch.sum(atomic_energies, dim=1))\n",
    "        gradient, = torch.autograd.grad(energy, [coords], create_graph=True)\n",
    "        return torch.hstack(energy), gradient\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qm_coord, atom_types, energy, gradient = batch\n",
    "        ene_pred, grad_pred = self(qm_coord, atom_types[0]) # training the energy and gradient predictions \n",
    "        ene_loss = F.mse_loss(ene_pred, energy)   # defining loss functions for energy and gradient predictions\n",
    "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
    "\n",
    "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
    "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
    "        w_ene = 1\n",
    "        w_grad = 1 + 99 * (lr / start_lr)  \n",
    "\n",
    "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('l2_trn', torch.sqrt(loss))\n",
    "        self.log('l2_e_trn', torch.sqrt(ene_loss))\n",
    "        self.log('l2_f_trn', torch.sqrt(grad_loss))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        torch.set_grad_enabled(True)\n",
    "        qm_coord, atom_types, energy, gradient = batch\n",
    "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
    "        ene_loss = F.mse_loss(ene_pred, energy)\n",
    "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
    "\n",
    "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
    "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
    "        w_ene = 1\n",
    "        w_grad = 1 + 99 * (lr / start_lr)\n",
    "\n",
    "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('l2_tst', torch.sqrt(loss))\n",
    "        self.log('l2_e_tst', torch.sqrt(ene_loss))\n",
    "        self.log('l2_f_tst', torch.sqrt(grad_loss))\n",
    "        self.log('lr', lr)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95),\n",
    "                     'interval': 'epoch',\n",
    "                     'frequency': 10,\n",
    "                    }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We will now load the files previously imported from GitHub and use the data in our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ds = np.DataSource(None) \n",
    "qm_coord = np.array(np.load(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/qm_coord.npy\", \"rb\")), dtype=\"float32\")\n",
    "atom_types = np.loadtxt(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/qm_elem.txt\", \"r\"), dtype=int)\n",
    "elems = np.unique(atom_types).tolist()\n",
    "atom_types = np.array([[elems.index(i) for i in atom_types]])\n",
    "atom_types = atom_types.repeat(len(qm_coord), axis=0)\n",
    "\n",
    "energy = np.array((np.load(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/energy.npy\", \"rb\")) - np.load(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/energy_sqm.npy\", \"rb\"))) * 27.2114 * 23.061, dtype=\"float32\")\n",
    "energy = energy - energy.mean()\n",
    "qm_gradient = np.array((np.load(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/qm_grad.npy\", \"rb\")) - np.load(ds.open(\"https://github.com/cc-ats/mlp_class/raw/main/Butane/qm_grad_sqm.npy\", \"rb\"))) * 27.2114 * 23.061 / 0.529177249, dtype=\"float32\")\n",
    "\n",
    "# Convert NumPy Arrays to Torch Tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "qm_coord = torch.from_numpy(qm_coord).to(device)\n",
    "atom_types = torch.from_numpy(atom_types).to(device)\n",
    "energy = torch.from_numpy(energy).to(device)\n",
    "qm_gradient = torch.from_numpy(qm_gradient).to(device)\n",
    "\n",
    "pl.seed_everything(2)\n",
    "dataset = TensorDataset(qm_coord, atom_types, energy, qm_gradient)\n",
    "train, val = random_split(dataset, [1728, 72])  \n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANI for Feature Extraction \n",
    "\n",
    "Here is an example where we use our ANI model for feature extraction. We will use values for our symmetry function parameters from ***cite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%time\n",
    "pl.seed_everything(2)\n",
    "\n",
    "ani = True\n",
    "\n",
    "if ani:\n",
    "  # From TorchANI\n",
    "  Rcr = 5.2000e+00\n",
    "  Rca = 3.5000e+00\n",
    "  EtaR = [1.6000000e+01]\n",
    "  ShfR = [9.0000000e-01,1.1687500e+00,1.4375000e+00,1.7062500e+00,1.9750000e+00,2.2437500e+00,2.5125000e+00,2.7812500e+00,3.0500000e+00,3.3187500e+00,3.5875000e+00,3.8562500e+00,4.1250000e+00,4.3937500e+00,4.6625000e+00,4.9312500e+00]\n",
    "  Zeta = [3.2000000e+01]\n",
    "  ShfZ = [1.9634954e-01,5.8904862e-01,9.8174770e-01,1.3744468e+00,1.7671459e+00,2.1598449e+00,2.5525440e+00,2.9452431e+00]\n",
    "  EtaA = [8.0000000e+00]\n",
    "  ShfA = [9.0000000e-01,1.5500000e+00,2.2000000e+00,2.8500000e+00]\n",
    "  EtaR, ShfR = np.array(np.meshgrid(EtaR, ShfR)).reshape(2, -1)\n",
    "  Zeta, ShfZ, EtaA, ShfA = np.array(np.meshgrid(Zeta, ShfZ, EtaA, ShfA)).reshape(4, -1)\n",
    "  EtaR = np.repeat([EtaR], 3, axis=0)\n",
    "  ShfR = np.repeat([ShfR], 3, axis=0)\n",
    "  Zeta = np.repeat([Zeta], 3, axis=0)\n",
    "  ShfZ = np.repeat([ShfZ], 3, axis=0)\n",
    "  EtaA = np.repeat([EtaA], 3, axis=0)\n",
    "  ShfA = np.repeat([ShfA], 3, axis=0)\n",
    "  descriptor = FeatureANI(Rcr, EtaR, ShfR, Rca, Zeta, ShfZ, EtaA, ShfA)\n",
    "else:\n",
    "  Rcr = 6.0\n",
    "  Rca = 6.0\n",
    "  ShfR = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]] # H, C, O\n",
    "  EtaR = [[0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79]] # H, C, O\n",
    "  Zeta = [[1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32]] # H, C, O\n",
    "  EtaA = [[0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79]] # H, C, O\n",
    "  descriptor = Feature(Rcr, EtaR, ShfR, Rca, Zeta, EtaA)\n",
    "\n",
    "fitting_net = Fitting(3, descriptor.output_length, neuron=[240, 240, 240])\n",
    "model = BPNN(descriptor, fitting_net, learning_rate=5e-4)\n",
    "csv_logger = pl_loggers.CSVLogger('logs_csv/')\n",
    "trainer = pl.Trainer(max_epochs=500, logger=csv_logger, log_every_n_steps=20, accelerator='gpu')\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model to PyTorch File\n",
    "The following files will be saved:\n",
    "\n",
    "**1) model.pt**\n",
    "- torch.save saves tensors to model.pt\n",
    "\n",
    "**2) model_script.pt**\n",
    "- torch.jit.save attempts to preserve the behavior of some operators across PyTorch versions.\n",
    "\n",
    "Previously saved models can be loaded with:\n",
    " - model.load_state_dict(torch.load(' 1) '))\n",
    " - torch.jit.load(' 2) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')\n",
    "torch.jit.save(model.to_torchscript(), \"model_script.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ene_pred, grad_pred = model(qm_coord.cpu(), atom_types[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "e1 = energy.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
    "e2 = ene_pred.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
    "ax[0].plot(e1, e2, linestyle='none', marker='.', )\n",
    "ax[0].plot([np.max(np.concatenate((e1,e2))), np.min(np.concatenate((e1,e2)))], [np.max(np.concatenate((e1,e2))), np.min(np.concatenate((e1,e2)))], color=\"k\", linewidth=1.5)\n",
    "ax[0].set_xlabel(\"Reference Energy (kcal/mol)\", size=14)\n",
    "ax[0].set_ylabel(\"Predicted Energy (kcal/mol)\", size=14)\n",
    "ax[0].annotate('RMSD: %.3f' % np.sqrt(np.mean((e1 - e2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
    "\n",
    "f1 = -qm_gradient.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
    "f2 = -grad_pred.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
    "\n",
    "ax[1].plot(f1, f2, linestyle='none', marker='.', )\n",
    "plt.plot([-np.abs(np.max(np.concatenate((f1,f2)))), np.max(np.concatenate((f1,f2)))], [-np.max(np.concatenate((f1,f2))), np.max(np.concatenate((f1,f2)))], color=\"k\", linewidth=1.5)\n",
    "ax[1].set_xlabel(\"Reference Force (kcal/mol/A)\", size=14)\n",
    "ax[1].set_ylabel(\"Predicted Force (kcal/mol/A)\", size=14)\n",
    "ax[1].annotate('RMSD: %.3f' % np.sqrt(np.mean((f1 - f2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rmsd.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('logs_csv/lightning_logs/version_0/metrics.csv')\n",
    "fig, ax = plt.subplots(2, figsize=(6,10))\n",
    "x = data['epoch'][~data['l2_e_tst'].isnull()]\n",
    "y = data['l2_e_tst'][~data['l2_e_tst'].isnull()]\n",
    "y2 = data['l2_f_tst'][~data['l2_f_tst'].isnull()]\n",
    "ax[0].semilogy(x, y)\n",
    "ax[0].set_ylabel('Energy Validation Error (kcal/mol)')\n",
    "ax[1].semilogy(x, y2, label='Force Validation Error (kcal/mol')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Force Validation Error (kcal/mol/A)')\n",
    "fig.savefig('loss.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('logs_csv/lightning_logs/version_0/metrics.csv')\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "x = data['epoch'][~data['epoch'].isnull()]\n",
    "y = data['train_loss'][~data['train_loss'].isnull()]\n",
    "print(len(y))\n",
    "plt.semilogy(y, label='Train Loss')\n",
    "y = data['val_loss'][~data['val_loss'].isnull()]\n",
    "print(len(y))\n",
    "plt.semilogy(y, label='Validation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
