{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Netwotk with Deep Potential\n",
    "<a href=\"https://colab.research.google.com/github/cc-ats/mlp_class/blob/main/ClaisenRearrangement_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Trajectory { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Reaction coordinate is $d_1 -d_2$ with 21 windows.\n",
    "\n",
    "#@markdown Total of 2100 frames (1 ps/window, frames are saved every 1 fs.)\n",
    "!rm /content/claisen.* &> /dev/null\n",
    "!rm /content/*.png &> /dev/null\n",
    "!pip install -U kora &> /dev/null\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/img/claisen.mp4 &> /dev/null\n",
    "from kora.drive import upload_public\n",
    "url = upload_public('claisen.mp4')\n",
    "# then display it\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(f\"\"\"<video src={url} width=300 controls/>\"\"\")\n",
    "\n",
    "#@markdown <img src='https://raw.githubusercontent.com/cc-ats/mlp_class/main/Claisen_Rearrangement/img/pent-4-enal.png' width=200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import data from GitHub (mlp_class) { display-mode: \"form\" }\n",
    "#@markdown Install PyTorch Lightning.\n",
    "\n",
    "#@markdown Files from GitHub:\n",
    "#@markdown - **qm_coord.npy** (2100, 14, 3)\n",
    "#@markdown - **qm_elem.txt** ([8, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "#@markdown - PM3\n",
    "#@markdown  - **energy_sqm.npy** (2100,)\n",
    "#@markdown  - **qm_grad_sqm.npy** (2100, 14, 3)\n",
    "\n",
    "#@markdown - B3LYP/6-31+G*\n",
    "#@markdown  - **energy.npy**  (2100,)\n",
    "#@markdown  - **qm_grad.npy** (2100, 14, 3)\n",
    "\n",
    "\n",
    "#@markdown - ml_qmmm.py (Feature and Fitting Neural Networks)\n",
    "\n",
    "%%capture\n",
    "!rm *py*\n",
    "!rm qm_elem.txt\n",
    "!rm -r sample_data\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy.npy\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy_sqm.npy\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad.npy\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad_sqm.npy\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_coord.npy\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_elem.txt\n",
    "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/ml_qmmm.py\n",
    "\n",
    "!pip install pytorch-lightning > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries \n",
    "\n",
    "#@markdown - math, typing (Sequence, Tuple)\n",
    "\n",
    "#@markdown - Torch (nn, nn.functional, Tensor. TensorDataset, DataLoader, random_split)\n",
    "\n",
    "#@markdown - PyTorch Lightning (loggers)\n",
    "\n",
    "#@markdown - ml_qmmm (Feature, Fitting)\n",
    "import math\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import ml_qmmm\n",
    "from ml_qmmm import Feature, Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPot(pl.LightningModule):\n",
    "    def __init__(self, descriptor: nn.Module, fitting_net: nn.Module, learning_rate=5e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.descriptor = descriptor\n",
    "        self.fitting_net = fitting_net\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, coords: torch.Tensor, atom_types: torch.Tensor):\n",
    "        coords.requires_grad_()\n",
    "        descriptors = self.descriptor(coords, atom_types)\n",
    "        atomic_energies = self.fitting_net((descriptors, atom_types))\n",
    "        energy = torch.unbind(torch.sum(atomic_energies, dim=1))\n",
    "        gradient, = torch.autograd.grad(energy, [coords], create_graph=True)\n",
    "        return torch.hstack(energy), gradient\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qm_coord, atom_types, energy, gradient = batch\n",
    "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
    "        ene_loss = F.mse_loss(ene_pred, energy)\n",
    "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
    "\n",
    "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
    "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
    "        w_ene = 1\n",
    "        w_grad = 1 + 99 * (lr / start_lr)\n",
    "\n",
    "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('l2_trn', torch.sqrt(loss))\n",
    "        self.log('l2_e_trn', torch.sqrt(ene_loss))\n",
    "        self.log('l2_f_trn', torch.sqrt(grad_loss))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        torch.set_grad_enabled(True)\n",
    "        qm_coord, atom_types, energy, gradient = batch\n",
    "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
    "        ene_loss = F.mse_loss(ene_pred, energy)\n",
    "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
    "\n",
    "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
    "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
    "        w_ene = 1\n",
    "        w_grad = 1 + 99 * (lr / start_lr)\n",
    "\n",
    "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('l2_tst', torch.sqrt(loss))\n",
    "        self.log('l2_e_tst', torch.sqrt(ene_loss))\n",
    "        self.log('l2_f_tst', torch.sqrt(grad_loss))\n",
    "        self.log('lr', lr)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95),\n",
    "                     'interval': 'epoch',\n",
    "                     'frequency': 10,\n",
    "                    }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "qm_coord = torch.from_numpy(np.array(np.load(\"qm_coord.npy\"), dtype=\"float32\")).cuda()\n",
    "atom_types = np.loadtxt(\"qm_elem.txt\", dtype=int)\n",
    "elems = np.unique(atom_types).tolist()\n",
    "atom_types = torch.from_numpy(np.array([elems.index(i) for i in atom_types])).cuda()\n",
    "atom_types = atom_types.repeat(len(qm_coord), 1)\n",
    "\n",
    "energy = torch.from_numpy(np.array((np.load(\"energy.npy\") - np.load(\"energy_sqm.npy\")) * 27.2114 * 23.061, dtype=\"float32\")).cuda()\n",
    "energy = energy - energy.mean()\n",
    "qm_gradient = torch.from_numpy(np.array((np.load(\"qm_grad.npy\") - np.load(\"qm_grad_sqm.npy\")) * 27.2114 * 23.061 / 0.529177249, dtype=\"float32\")).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(2)\n",
    "dataset = TensorDataset(qm_coord, atom_types, energy, qm_gradient)\n",
    "train, val = random_split(dataset, [2016, 84])\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pl.seed_everything(2)\n",
    "descriptor = Feature(3, neuron=[25, 50, 100], axis_neuron=4)\n",
    "fitting_net = Fitting(3, descriptor.output_length)\n",
    "model = DeepPot(descriptor, fitting_net, learning_rate=5e-4)\n",
    "csv_logger = pl_loggers.CSVLogger('logs_csv/')\n",
    "trainer = pl.Trainer(max_epochs=500, logger=csv_logger, log_every_n_steps=20, accelerator='gpu')\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Saving model to PyTorch file\n",
    "#@markdown The following files are saved:\n",
    "\n",
    "#@markdown **1) model.pt**\n",
    "#@markdown - torch.save saves tensors to model.pt\n",
    "\n",
    "#@markdown **2) model_script.pt**\n",
    "#@markdown - torch.jit.save attempts to preserve the behavior of some operators across PyTorch versions.\n",
    "\n",
    "#@markdown Previously saved models can be loaded with:\n",
    "#@markdown - model.load_state_dict(torch.load(' **1)** '))\n",
    "#@markdown - torch.jit.load(' **2)** ')\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "torch.jit.save(model.to_torchscript(), \"model_script.pt\")\n",
    "\n",
    "# To load models:\n",
    "# model.load_state_dict(torch.load('model_diff.pt'))\n",
    "# model = torch.jit.load('model_diff_script.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ene_pred, grad_pred = model(qm_coord.cpu(), atom_types[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "e1 = energy.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
    "e2 = ene_pred.detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
    "ax[0].plot(e1, e2, linestyle='none', marker='.', )\n",
    "ax[0].plot([np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], [np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], color=\"k\", linewidth=1.5)\n",
    "ax[0].set_xlabel(\"Reference Energy (kcal/mol)\", size=14)\n",
    "ax[0].set_ylabel(\"Predicted Energy (kcal/mol)\", size=14)\n",
    "ax[0].annotate('RMSD: %.3f' % np.sqrt(np.mean((e1 - e2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
    "\n",
    "f1 = -qm_gradient.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
    "f2 = -grad_pred.detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
    "\n",
    "ax[1].plot(f1, f2, linestyle='none', marker='.', )\n",
    "plt.plot([-np.abs(np.max(np.concatenate((f1,f2)))), np.max(np.concatenate((f1,f2)))], [-np.max(np.concatenate((f1,f2))), np.max(np.concatenate((f1,f2)))], color=\"k\", linewidth=1.5)\n",
    "ax[1].set_xlabel(\"Reference Force (kcal/mol/A)\", size=14)\n",
    "ax[1].set_ylabel(\"Predicted Force (kcal/mol/A)\", size=14)\n",
    "ax[1].annotate('RMSD: %.3f' % np.sqrt(np.mean((f1 - f2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rmsd.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('logs_csv/lightning_logs/version_0/metrics.csv')\n",
    "# data = pd.read_csv('/content/drive/MyDrive/f10_e500/logs_csv/lightning_logs/version_2/metrics.csv')\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "x = data['epoch'][~data['epoch'].isnull()]\n",
    "y = data['train_loss'][~data['train_loss'].isnull()]\n",
    "print(len(y))\n",
    "plt.semilogy(y, label='Train Loss')\n",
    "y = data['val_loss'][~data['val_loss'].isnull()]\n",
    "print(len(y))\n",
    "plt.semilogy(y, label='Validation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
