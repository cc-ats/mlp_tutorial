{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cc-ats/mlp_class/blob/cw_cl/Lesson6_DeepPot-FNN_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson 6: DeepPot-Smooth Edition Fitting Neural Network with Machine Learning Potentials (DeepPot-SE-FNN MLP)**\n",
        "\n",
        "$\\Delta$ MLP  with PyTorch for the claisen rearrangement reaction\n",
        "\n",
        "For this tutorial, we will be combining the Fitting Neural Network (FNN) from Lesson 1 and the DeepPot-Smooth Edition (DeepPot-SE) from Lesson 4 to train a $\\Delta$ Machine Learning Potential ($\\Delta$MLP) model to reproduce the energy and forces for the claisen rearrangement reaction. With a DeepPot-SE-FNN, we can utilize the properties from the local environment defined in the DeepPot-SE model for feature extraction and use the FNN for training. The goal of this model is to train with data that is from semiempirical (PM3) and DFT (B3LYP) levels of theory. The DeepPot-SE-FNN will be used to correct the semiempirical values to obtain DFT level accuracy, which is what makes it a $\\Delta$MLP model."
      ],
      "metadata": {
        "id": "d5swkZB6PnNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.1)Defining the Reaction Coordinate and System Size*** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Reaction coordinate is $d_1 -d_2$ with 21 windows.\n",
        "\n",
        "#@markdown Total of 2100 frames (1 ps/window, frames are saved every 1 fs.)\n",
        "!rm /content/claisen.* &> /dev/null\n",
        "!rm /content/*.png &> /dev/null\n",
        "!pip install -U kora &> /dev/null\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/img/claisen.mp4 &> /dev/null\n",
        "from kora.drive import upload_public\n",
        "url = upload_public('claisen.mp4')\n",
        "# then display it\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(f\"\"\"<video src={url} width=300 controls/>\"\"\")\n",
        "\n",
        "#@markdown <img src='https://raw.githubusercontent.com/cc-ats/mlp_class/main/Claisen_Rearrangement/img/pent-4-enal.png' width=200px>"
      ],
      "metadata": {
        "id": "gq1i82xllZNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXqPJT3Kg8EP"
      },
      "outputs": [],
      "source": [
        "#@title ***(6.2) Import Data from GitHub (mlp_class)*** { display-mode: \"form\" }\n",
        "#@markdown Install PyTorch Lightning.\n",
        "\n",
        "#@markdown Files from GitHub:\n",
        "#@markdown - **qm_coord.npy** (2100, 14, 3)\n",
        "#@markdown - **qm_elem.txt** ([8, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "#@markdown - PM3\n",
        "#@markdown  - **energy_sqm.npy** (2100,)\n",
        "#@markdown  - **qm_grad_sqm.npy** (2100, 14, 3)\n",
        "\n",
        "#@markdown - B3LYP/6-31+G*\n",
        "#@markdown  - **energy.npy**  (2100,)\n",
        "#@markdown  - **qm_grad.npy** (2100, 14, 3)\n",
        "\n",
        "\n",
        "#@markdown - ml_qmmm.py (Feature and Fitting Neural Networks)\n",
        "\n",
        "%%capture\n",
        "!rm *py*\n",
        "!rm qm_elem.txt\n",
        "!rm -r sample_data\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy_sqm.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad_sqm.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_coord.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_elem.txt\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/ml_qmmm.py\n",
        "\n",
        "!pip install pytorch-lightning > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.3) Import Libraries***\n",
        "\n",
        "#@markdown - math, typing (Sequence, Tuple)\n",
        "\n",
        "#@markdown - Torch (nn, nn.functional, Tensor. TensorDataset, DataLoader, random_split)\n",
        "\n",
        "#@markdown - PyTorch Lightning (loggers)\n",
        "\n",
        "#@markdown - ml_qmmm (Feature, Fitting)\n",
        "import math\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "import ml_qmmm\n",
        "from ml_qmmm import Feature, Fitting"
      ],
      "metadata": {
        "id": "l7wPLp-sg9Nm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.4) The DeepPot Class***\n",
        "#@markdown We define the DeepPot class to extract features from a data set using \n",
        "#@markdown local environment propertiess for training a neural network model. \n",
        "\n",
        "class DeepPot(pl.LightningModule):\n",
        "    def __init__(self, descriptor: nn.Module, fitting_net: nn.Module, learning_rate=5e-4) -> None:\n",
        "        super().__init__()\n",
        "        self.descriptor = descriptor\n",
        "        self.fitting_net = fitting_net\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, coords: torch.Tensor, atom_types: torch.Tensor):\n",
        "        coords.requires_grad_()\n",
        "        descriptors = self.descriptor(coords, atom_types)\n",
        "        atomic_energies = self.fitting_net((descriptors, atom_types))\n",
        "        energy = torch.unbind(torch.sum(atomic_energies, dim=1))\n",
        "        gradient, = torch.autograd.grad(energy, [coords], create_graph=True)\n",
        "        return torch.hstack(energy), gradient\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        qm_coord, atom_types, energy, gradient = batch\n",
        "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
        "        ene_loss = F.mse_loss(ene_pred, energy)\n",
        "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
        "\n",
        "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
        "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
        "        w_ene = 1\n",
        "        w_grad = 1 + 99 * (lr / start_lr)\n",
        "\n",
        "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('l2_trn', torch.sqrt(loss))\n",
        "        self.log('l2_e_trn', torch.sqrt(ene_loss))\n",
        "        self.log('l2_f_trn', torch.sqrt(grad_loss))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        torch.set_grad_enabled(True)\n",
        "        qm_coord, atom_types, energy, gradient = batch\n",
        "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
        "        ene_loss = F.mse_loss(ene_pred, energy)\n",
        "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
        "\n",
        "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
        "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
        "        w_ene = 1\n",
        "        w_grad = 1 + 99 * (lr / start_lr)\n",
        "\n",
        "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('l2_tst', torch.sqrt(loss))\n",
        "        self.log('l2_e_tst', torch.sqrt(ene_loss))\n",
        "        self.log('l2_f_tst', torch.sqrt(grad_loss))\n",
        "        self.log('lr', lr)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95),\n",
        "                     'interval': 'epoch',\n",
        "                     'frequency': 10,\n",
        "                    }\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "xM95a2o2hGVf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.5) Importing Data and Feature Extraction***\n",
        "#@markdown We can load in our semiempirical and DFT data, which will be used by \n",
        "#@markdown the DeepPot model for feature extraction. \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "qm_coord = torch.from_numpy(np.array(np.load(\"qm_coord.npy\"), dtype=\"float32\")).cuda()\n",
        "atom_types = np.loadtxt(\"qm_elem.txt\", dtype=int)\n",
        "elems = np.unique(atom_types).tolist()\n",
        "atom_types = torch.from_numpy(np.array([elems.index(i) for i in atom_types])).cuda()\n",
        "atom_types = atom_types.repeat(len(qm_coord), 1)\n",
        "\n",
        "energy = torch.from_numpy(np.array((np.load(\"energy.npy\") - np.load(\"energy_sqm.npy\")) * 27.2114 * 23.061, dtype=\"float32\")).cuda()\n",
        "energy = energy - energy.mean()\n",
        "qm_gradient = torch.from_numpy(np.array((np.load(\"qm_grad.npy\") - np.load(\"qm_grad_sqm.npy\")) * 27.2114 * 23.061 / 0.529177249, dtype=\"float32\")).cuda()"
      ],
      "metadata": {
        "id": "tDm7RmYwns4J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.seed_everything(2)\n",
        "dataset = TensorDataset(qm_coord, atom_types, energy, qm_gradient)\n",
        "train, val = random_split(dataset, [2016, 84])\n",
        "train_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ],
      "metadata": {
        "id": "NuF4V-SznyyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.6) Training the Neural Network***\n",
        "#@markdown Now we begin training our NN using the training and validation datasets. \n",
        "\n",
        "%%time\n",
        "pl.seed_everything(2)\n",
        "descriptor = Feature(3, neuron=[25, 50, 100], axis_neuron=4)\n",
        "fitting_net = Fitting(3, descriptor.output_length)\n",
        "model = DeepPot(descriptor, fitting_net, learning_rate=5e-4)\n",
        "csv_logger = pl_loggers.CSVLogger('logs_csv/')\n",
        "trainer = pl.Trainer(max_epochs=500, logger=csv_logger, log_every_n_steps=20, accelerator='gpu')\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "1uJoAwQNoGBk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.7) Saving the Model to a PyTorch File***\n",
        "#@markdown The following files are saved:\n",
        "\n",
        "#@markdown **1) model.pt**\n",
        "#@markdown - torch.save saves tensors to model.pt\n",
        "\n",
        "#@markdown **2) model_script.pt**\n",
        "#@markdown - torch.jit.save attempts to preserve the behavior of some operators across PyTorch versions.\n",
        "\n",
        "#@markdown Previously saved models can be loaded with:\n",
        "#@markdown - model.load_state_dict(torch.load(' **1)** '))\n",
        "#@markdown - torch.jit.load(' **2)** ')\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pt')\n",
        "torch.jit.save(model.to_torchscript(), \"model_script.pt\")\n",
        "\n",
        "# To load models:\n",
        "# model.load_state_dict(torch.load('model_diff.pt'))\n",
        "# model = torch.jit.load('model_diff_script.pt')"
      ],
      "metadata": {
        "id": "4XkVxKCQoJ4Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ene_pred, grad_pred = model(qm_coord.cpu(), atom_types[0].cpu())"
      ],
      "metadata": {
        "id": "xYgBIF-LpXi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.8) Plotting RMSD for $\\Delta$MLP Energy and Forces***\n",
        "#@markdown RMSD for predicted and reference energy and forces are calculated and\n",
        "#@markdown displayed below. \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "e1 = energy.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
        "e2 = ene_pred.detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
        "ax[0].plot(e1, e2, linestyle='none', marker='.', )\n",
        "ax[0].plot([np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], [np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], color=\"k\", linewidth=1.5)\n",
        "ax[0].set_xlabel(\"Reference Energy (kcal/mol)\", size=14)\n",
        "ax[0].set_ylabel(\"Predicted Energy (kcal/mol)\", size=14)\n",
        "ax[0].annotate('RMSD: %.3f' % np.sqrt(np.mean((e1 - e2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
        "\n",
        "f1 = -qm_gradient.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
        "f2 = -grad_pred.detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
        "\n",
        "ax[1].plot(f1, f2, linestyle='none', marker='.', )\n",
        "plt.plot([-np.abs(np.max(np.concatenate((f1,f2)))), np.max(np.concatenate((f1,f2)))], [-np.max(np.concatenate((f1,f2))), np.max(np.concatenate((f1,f2)))], color=\"k\", linewidth=1.5)\n",
        "ax[1].set_xlabel(\"Reference Force (kcal/mol/A)\", size=14)\n",
        "ax[1].set_ylabel(\"Predicted Force (kcal/mol/A)\", size=14)\n",
        "ax[1].annotate('RMSD: %.3f' % np.sqrt(np.mean((f1 - f2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rmsd.png', dpi=300)"
      ],
      "metadata": {
        "id": "qedW96hrpZFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.9) The Model Weights and Biases Dictionary***\n",
        "#@markdown This cell prints the size of the weights and biases\n",
        "#@markdown used in the trained model for reference.\n",
        "\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "id": "RQ49pMPzQ4xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***(6.10) Plotting Training and Validation Loss for $\\Delta$MLP***\n",
        "#@markdown The loss at each step of the training process is displayed below. \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('logs_csv/lightning_logs/version_0/metrics.csv')\n",
        "# data = pd.read_csv('/content/drive/MyDrive/f10_e500/logs_csv/lightning_logs/version_2/metrics.csv')\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "x = data['epoch'][~data['epoch'].isnull()]\n",
        "y = data['train_loss'][~data['train_loss'].isnull()]\n",
        "print(len(y))\n",
        "plt.semilogy(y, label='Train Loss')\n",
        "y = data['val_loss'][~data['val_loss'].isnull()]\n",
        "print(len(y))\n",
        "plt.semilogy(y, label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png', dpi=300)"
      ],
      "metadata": {
        "id": "N1XKrNWF7yMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GtbJcSaTLifb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDauBtYGj27N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}