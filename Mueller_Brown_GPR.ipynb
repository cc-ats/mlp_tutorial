{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the definition of Mueller-Brown potential, see https://www.wolframcloud.com/objects/demonstrations/TrajectoriesOnTheMullerBrownPotentialEnergySurface-source.nb.\n",
    "\n",
    "$v(x,y) = \\sum_{k=0}^3 A_k \\mathrm{exp}\\left[ a_k (x - x_k^0)^2 + b_k (x - x_k^0) (y - y_k^0) + c_k (y - y_k^0)^2 \\right] $\n",
    "\n",
    "**Generate Traning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture hides the output\n",
    "%%capture \n",
    "!pip3 install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, pow, tanh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mueller_brown_potential(x, y):\n",
    "  A = [-200, -100, -170, 15]\n",
    "  a = [-1, -1, -6.5, 0.7]\n",
    "  b = [0, 0, 11, 0.6]\n",
    "  c = [-10, -10, -6.5, 0.7]\n",
    "  x0 = [1, 0, -0.5, -1.0]\n",
    "  y0 = [0, 0.5, 1.5, 1]\n",
    "  value = 0\n",
    "  for k in range(0, 4):\n",
    "    value += 0.1 * A[k] * exp( a[k] * pow(x-x0[k], 2.0) + b[k] * (x-x0[k]) * (y-y0[k]) + c[k] * pow(y-y0[k], 2.0))\n",
    "  return value\n",
    "\n",
    "#main function\n",
    "xx = np.arange(-1.8, 1.4, 0.1)\n",
    "yy = np.arange(-0.4, 2.4, 0.1)\n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "xy, xy_truncated = [],[]\n",
    "z, z_truncated = [], []\n",
    "for y in yy:\n",
    "  for x in xx:\n",
    "    v = mueller_brown_potential(x,y)\n",
    "    z.append(v)\n",
    "    xy.append([x,y])\n",
    "    if v < 10:  #keep only low-energy points for ML training\n",
    "      xy_truncated.append([x, y])\n",
    "      z_truncated.append(v)\n",
    "Z = np.reshape(z,(len(yy),-1))\n",
    "print(\"Zmin:\", np.amin(Z), \"Zmax:\", np.amax(Z))\n",
    "print(\"Size of (future) training set:\", len(z_truncated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-d Projection Surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "import copy\n",
    "\n",
    "fig = plt.figure(figsize=(6,3), dpi=150)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Made copy of Z just to clean up the plot.\n",
    "clean_Z = copy.copy(Z)\n",
    "clean_Z[clean_Z>35] = np.nan\n",
    "print(np.amax(clean_Z), np.amin(clean_Z))\n",
    "surf=ax.plot_surface(X, Y, clean_Z, rstride=1, cstride=1,\n",
    "                cmap=plt.cm.coolwarm, edgecolor='none', vmin=-15, vmax=40)\n",
    "\n",
    "ax.view_init(30,-30)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title('Mueller Brown Surface')\n",
    "plt.colorbar(surf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contour surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,5), dpi=100) \n",
    "levels = [-12, -10, -8, -6, -4, -2, 0, 4, 8] \n",
    "ct = plt.contour(X, Y, Z, levels, colors='k') \n",
    "plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8) \n",
    "ct = plt.contourf(X, Y, Z, levels, cmap=plt.cm.rainbow, extend='both', vmin=-15, vmax=0) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"y\") \n",
    "plt.colorbar() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading PyTorch, GpyTorch, and training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpytorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch import Tensor\n",
    "\n",
    "#dataset = TensorDataset(Tensor(xy_truncated), Tensor(z_truncated))\n",
    "#train_loader = DataLoader(dataset, batch_size=32)\n",
    "#print(\"Size of training set:\", len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the variables in our function as a vector of input features $\\textbf{x}=[x,y]$ \n",
    "\n",
    "A set of $n$ configurations can be assembled in to a training set $\\textbf{X}=[\\textbf{x}_1, ...,\\textbf{x}_n]$ with a set of observations $\\textbf{y}=[y_1,...,y_n]$. For noisy samples, we can assume that an observations is seperate from the underlying function according to $y=f(\\textbf{x})+\\mathit{ε}$, where the noise, $\\mathit{ε}$, follows a gaussian distribution $\\mathit{ε}\\sim\\mathcal{N}(0,σ^2_n)$. The prior distribution of underlying functions follows a Gaussian distribution $\\textbf{f}(\\textbf{X})\\sim\\mathcal{N}(\\textbf{0}, \\textbf{K}(\\textbf{X},\\textbf{X}))$, where $\\textbf{0}$ is the mean function and $\\textbf{K}$ is the covaraince kernel matrix. The covariance kernel matrix is assembled based on a kernel function, $k$, that is used to compare the simularities between input vectors:\n",
    "\n",
    "$\\textbf{K(X,X)}=\n",
    "\\begin{bmatrix}\n",
    "k(\\textbf{x}_1,\\textbf{x}_1) & \\ldots & k(\\textbf{x}_1,\\textbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "k(\\textbf{x}_n,\\textbf{x}_1) & \\ldots & k(\\textbf{x}_n,\\textbf{x}_n)\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Here we used the radial basis function:\n",
    "\n",
    "$\\mathrm{k}(\\textbf{x}_a,\\textbf{x}_b)=\\sigma^2_f\\mathrm{exp}(-\\frac{||\\textbf{x}_a-\\textbf{x}_b||^2}{2l^2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GPR Model: Taken directly From gpytorch tutorial with minor changes \n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        #super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        #self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "X_gpr = Tensor(xy_truncated)\n",
    "Y_gpr = Tensor(z_truncated)\n",
    "\n",
    "# Initialize Likelihood and Model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_gpr, Y_gpr, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\textbf{K}=\\textbf{K}(\\textbf{X},\\textbf{X})+σ^2_n\\textbf{I}$\n",
    "\n",
    "Hyperparameters $\\Theta = \\{σ^2_f, l, \\sigma^2_n\\}$ are optimized by maximizing the marginal likelihood log: \n",
    "\n",
    "$\\mathrm{log}\\:p(\\textbf{y}|\\textbf{X},\\textbf{Θ})=-\\frac{1}{2}\\textbf{y}^\\mathrm{T}\\textbf{K}^{-1}\\textbf{y}-\\frac{1}{2}\\mathrm{log}\\:|\\textbf{K}|-\\frac{n}{2}\\mathrm{log}\\:2\\pi$\n",
    "\n",
    "To demonstrate that the negative marginal likelihood log function is smooth, the noise hyperparameter, $\\sigma^2_n$ is fixed, while a grid search is performed over the lengthscale, $l$, and output/scale variance, $σ^2_f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_value = 1.0\n",
    "scale_and_length = [[i*.1,j*.1] for i in range(1,50) for j in range(1,50)]\n",
    "\n",
    "x_plt = []\n",
    "y_plt = []\n",
    "z_plt = []\n",
    "\n",
    "for pair in scale_and_length:\n",
    "    hypers = {\n",
    "        'likelihood.noise_covar.noise': torch.tensor(noise_value),\n",
    "        'covar_module.base_kernel.lengthscale': torch.tensor(pair[0]),\n",
    "        'covar_module.outputscale': torch.tensor(pair[1]),\n",
    "    }\n",
    "    model.initialize(**hypers)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    output = model(X_gpr)\n",
    "    loss = -mll(output, Y_gpr)\n",
    "    x_plt.append(pair[0])\n",
    "    y_plt.append(pair[1])\n",
    "    z_plt.append(loss.item())\n",
    "\n",
    "fig = plt.figure(figsize=(3,3), dpi=200)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "ct = plt.tricontour(x_plt, y_plt, z_plt, colors='k')\n",
    "plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8)\n",
    "ct = plt.tricontourf(x_plt, y_plt, z_plt, cmap=plt.cm.rainbow, extend='both')\n",
    "plt.title(\"- Marginal Likelihood Log\")\n",
    "plt.xlabel(\"lengthscale\")\n",
    "plt.ylabel(\"outputscale\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "def train_model(model, likelihood, print_hp=False):\n",
    "  hypers = {\n",
    "      'likelihood.noise_covar.noise': torch.tensor(1.0),\n",
    "      'covar_module.base_kernel.lengthscale': torch.tensor(1.0),\n",
    "      'covar_module.outputscale': torch.tensor(1.0),\n",
    "  }\n",
    "  model.initialize(**hypers)\n",
    "  if print_hp:\n",
    "      # Print untrained hyperparameters\n",
    "      for param_name, param in model.named_parameters():\n",
    "        print(f'Parameter name: {param_name:42} value = {param.item()}')\n",
    "\n",
    "  training_iter = 100\n",
    "  # Find optimal model hyperparameters\n",
    "  # Use the adam optimizer\n",
    "  #optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "  # Use the SGD optimizer \n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "  # \"Loss\" for GPs - the marginal log likelihood\n",
    "  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "  for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_gpr)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_gpr)\n",
    "    loss.backward()\n",
    "    if print_hp:\n",
    "      print('Iter %d/%d - Loss: %.3f scale: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.outputscale.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "      ))\n",
    "      optimizer.step()\n",
    "  if print_hp:\n",
    "    # Print Trained hyperparameters\n",
    "    for param_name, param in model.named_parameters():\n",
    "      print(f'Parameter name: {param_name:42} value = {param.item()}')\n",
    "\n",
    "train_model(model, likelihood, print_hp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model can be used to make predictions on new input space, $\\textbf{x}_*$. Predictions will follow a gaussian distribution $\\mathcal{N}\\sim(μ_*,\\textbf{Σ}_*)$, where $μ^*=\\textbf{K}^\\mathrm{T}_\\ast\\textbf{K}^{-1}\\textbf{y}$ and $\\textbf{Σ}_*=\\textbf{K}_{**}-\\textbf{K}^\\mathrm{T}_*\\textbf{K}^{-1}\\textbf{K}_*$. In the above equaiton, $\\textbf{K}_*=\\textbf{K}(\\textbf{X},\\textbf{x}_*)$ and $\\textbf{K}_{**}=\\textbf{K}(\\textbf{x}_*,\\textbf{x}_*)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predicted surface, and compare to the reference one\n",
    "def show_surface(model):\n",
    "  model.eval()\n",
    "  z_pred = model(Tensor(xy)).mean\n",
    "  z_var = model(Tensor(xy)).variance\n",
    "  Z_pred = np.reshape(z_pred.detach().numpy(),(len(yy),-1))\n",
    "  Z_var = np.reshape(z_var.detach().numpy(),(len(yy),-1))\n",
    "  Zdiff = np.subtract(Z_pred, Z)\n",
    "  fig = plt.figure(figsize=(6,6), dpi=200)\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  levels = [-12, -10, -8, -6, -4, -2, 0, 4, 8]\n",
    "  ct = plt.contour(X, Y, Z_pred, levels, colors='k')\n",
    "  plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8)\n",
    "  ct = plt.contourf(X, Y, Z_pred, levels, cmap=plt.cm.rainbow, extend='both', vmin=-15, vmax=0)\n",
    "  plt.title(\"Predicted\")\n",
    "  plt.xlabel(\"x\")\n",
    "  plt.ylabel(\"y\")\n",
    "  plt.colorbar()\n",
    "\n",
    "  plt.subplot(2, 2, 2)\n",
    "  levels = [-12, -10, -8, -6, -4, -2, 0, 4, 8]\n",
    "  ct = plt.contour(X, Y, Z, levels, colors='k')\n",
    "  plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8)\n",
    "  ct = plt.contourf(X, Y, Z, levels, cmap=plt.cm.rainbow, extend='both', vmin=-15, vmax=0)\n",
    "  plt.title(\"Reference\")\n",
    "  plt.xlabel(\"x\")\n",
    "  plt.ylabel(\"y\")\n",
    "  plt.colorbar()\n",
    "\n",
    "  plt.subplot(2, 2, 3)\n",
    "  levels = [-4, -2, 0, 2, 4]\n",
    "  ct = plt.contour(X, Y, Zdiff, levels, colors='k')\n",
    "  plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8)\n",
    "  ct = plt.contourf(X, Y, Zdiff, levels, cmap=plt.cm.rainbow, extend='both', vmin=-4, vmax=4)\n",
    "  plt.title(\"Difference\")\n",
    "  plt.xlabel(\"x\")\n",
    "  plt.ylabel(\"y\")\n",
    "  print(\"diff, min, max:\", np.amin(Zdiff), np.amax(Zdiff))\n",
    "  plt.colorbar()\n",
    "\n",
    "  plt.subplot(2, 2, 4)\n",
    "  levels = [0, 1, 2, 3, 4]\n",
    "  ct = plt.contour(X, Y, Zdiff, levels, colors='k')\n",
    "  plt.clabel(ct, inline=True, fmt='%3.0f', fontsize=8)\n",
    "  ct = plt.contourf(X, Y, Z_var, levels, cmap=plt.cm.rainbow, extend='both', vmin=0, vmax=4)\n",
    "  plt.title(\"Variance\")\n",
    "  plt.xlabel(\"x\")\n",
    "  plt.ylabel(\"y\")\n",
    "  print(\"var, min, max:\", np.amin(Z_var), np.amax(Z_var))\n",
    "  plt.colorbar()\n",
    "\n",
    "  plt.tight_layout()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "show_surface(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance and Training Set Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of (current) training set:\", Y_gpr.shape[0])\n",
    "X_test = X_gpr.detach()\n",
    "Y_test = Y_gpr.detach()\n",
    "def evaluate_model(train_x, train_y, test_x, test_y, model):\n",
    "  model.eval()\n",
    "  preds_train = model(train_x).mean\n",
    "  preds_test = model(test_x).mean\n",
    "  print(train_y.shape, preds_train.shape)\n",
    "  rmse_train = torch.sqrt(torch.mean(train_y - preds_train))\n",
    "  rmse_test = torch.sqrt(torch.mean(test_y - preds_test))\n",
    "  r2 = 1 - torch.sum((train_y-preds_train)**2)/torch.sum((train_y-torch.mean(train_y))**2)\n",
    "  q2 = 1 - torch.sum((train_y-preds_train)**2)/torch.sum((train_y-torch.mean(train_y))**2)\n",
    "  print(\"Training Data - RMSE: %.3f R^2: %3f\" % (rmse_train, r2))\n",
    "  print(\"Teseting Data - RMSE: %.3f R^2: %3f\" % (rmse_test, q2))\n",
    "  return rmse_train, r2, rmse_test, q2\n",
    "\n",
    "def reduce_training_set(train_x, train_y, new_size):\n",
    "  arr_index = np.arange(train_y.shape[0])\n",
    "  np.random.shuffle(arr_index)\n",
    "  return train_x[arr_index[:new_size],:], train_y[arr_index[:new_size]]\n",
    "\n",
    "def new_model(train_x, train_y):\n",
    "  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "  model = ExactGPModel(train_x, train_y, likelihood)\n",
    "  model.train()\n",
    "  likelihood.train()\n",
    "  train_model(model, likelihood, print_hp=False)\n",
    "  return model\n",
    "\n",
    "size_list = []\n",
    "rmse_train_list = []\n",
    "r2_list = []\n",
    "rmse_test_list = []\n",
    "q2_list = []\n",
    "\n",
    "training_set_sizes = [696, 600, 500, 400, 300, 200, 100]\n",
    "for set_size in training_set_sizes:\n",
    "  X_gpr, Y_gpr = reduce_training_set(X_gpr, Y_gpr, set_size)\n",
    "  model = new_model(X_gpr, Y_gpr)\n",
    "  rmse_train, r2, rmse_test, q2 = evaluate_model(X_gpr, Y_gpr, X_test, Y_test, model)\n",
    "  size_list.append(set_size)\n",
    "  rmse_train_list.append(rmse_train)\n",
    "  r2_list.append(r2)\n",
    "  rmse_test_list.append(rmse_test)\n",
    "  q2_list.append(q2)\n",
    "\n",
    "training_set_dict = {\n",
    "    'Training Set Size': size_list,\n",
    "    'Training Set RMSE': rmse_train_list,\n",
    "    'R^2': r2_list,\n",
    "    'Testing Set RMSE': rmse_test_list,\n",
    "    'Q^2': q2_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "print(tabulate.tabulate(training_set_dict, headers = 'keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comaparing Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GPR Model: Taken directly From gpytorch tutorial with minor changes \n",
    "class GPModel_kernel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        #super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "X_gpr = Tensor(xy_truncated) \n",
    "Y_gpr = Tensor(z_truncated)\n",
    "\n",
    "kernels = [\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel()),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel())    \n",
    "]\n",
    "\n",
    "def train_model_k(model, likelihood):\n",
    "  training_iter = 50\n",
    "  # Use the SGD optimizer \n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "  # \"Loss\" for GPs - the marginal log likelihood\n",
    "  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "  for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_gpr)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_gpr)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "for kernel in kernels:\n",
    "  # Initialize Likelihood and Model\n",
    "  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "  model = GPModel_kernel(X_gpr, Y_gpr, likelihood, kernel)\n",
    "  model.train()\n",
    "  likelihood.train()\n",
    "  train_model_k(model, likelihood)\n",
    "  rmse_train, r2, rmse_test, q2 = evaluate_model(X_gpr, Y_gpr, X_test, Y_test, model)\n",
    "  print(rmse_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyJBCn3gtnqNnll3RjQ/pq",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
