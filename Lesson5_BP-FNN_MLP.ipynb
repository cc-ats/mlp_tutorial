{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cc-ats/mlp_class/blob/cw_cl/ClaisenRearrangement_BP_PyTorch_CL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QIfKc-Me5Ni"
      },
      "source": [
        "# **Lesson 5: Behler-Parrinello Fitting Neural Network with Machine Learning Potentials  (BP-FNN MLP)** \n",
        "\n",
        "For this tutorial, we will be combining the Fitting Neural Network (FNN) from Lesson 1 and the Behler-Parrinello Neural Network (BPNN) from Lesson 3 to train a $\\Delta$ Machine Learning Potential ($\\Delta$MLP) model to reproduce the energy and forces for the claisen rearrangement reaction. With a BP-FNN, we can utilize the symmetry functions from the BPNN model for feature extraction and use the FNN for training. The goal of this model is to train with data that is from the semiempirical (PM3) and DFT (B3LYP) levels of theory. The BP-FNN will be used to correct the semiempirical values to obtain DFT level accuracy, which is what makes it a $\\Delta$MLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuHEYr6CCf5u"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.1) Import data from GitHub (mlp_class)*** { display-mode: \"form\" }\n",
        "#@markdown Install PyTorch Lightning.\n",
        "\n",
        "#@markdown Files from GitHub:\n",
        "#@markdown - **qm_coord.npy** (2100, 14, 3)\n",
        "#@markdown - **qm_elem.txt** ([8, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "#@markdown - PM3\n",
        "#@markdown  - **energy_sqm.npy** (2100,)\n",
        "#@markdown  - **qm_grad_sqm.npy** (2100, 14, 3)\n",
        "\n",
        "#@markdown - B3LYP/6-31+G*\n",
        "#@markdown  - **energy.npy**  (2100,)\n",
        "#@markdown  - **qm_grad.npy** (2100, 14, 3)\n",
        "\n",
        "%%capture\n",
        "!rm *py*\n",
        "!rm qm_elem.txt\n",
        "!rm -r sample_data\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/energy_sqm.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_grad_sqm.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_coord.npy\n",
        "!wget https://github.com/cc-ats/mlp_class/raw/main/Claisen_Rearrangement/qm_elem.txt\n",
        "\n",
        "!pip install pytorch-lightning > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IsPCKD1DCUmf"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.2) Import libraries*** \n",
        "\n",
        "#@markdown - math, typing (Sequence, Tuple)\n",
        "\n",
        "#@markdown - Torch (nn, nn.functional, Tensor. TensorDataset, DataLoader, random_split)\n",
        "\n",
        "#@markdown - PyTorch Lightning (loggers)\n",
        "\n",
        "import math\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXspRElYvr5n"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.3) Defining the Dense Neural Network***\n",
        "#@markdown The Dense class contains functions to describe a densely connected neural network.\n",
        "\n",
        "class Sequential(nn.Sequential):\n",
        "    def forward(self, input: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n",
        "        for module in self:\n",
        "            input = module(input)\n",
        "        return input\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, num_channels: int, in_features: int, out_features: int, bias: bool = True, activation: bool = False, residual: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(num_channels, out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_channels, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.activation = activation\n",
        "        self.residual = residual\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w in self.weight:\n",
        "            nn.init.kaiming_uniform_(w, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            for b, w in zip(self.bias, self.weight):\n",
        "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w)\n",
        "                bound = 1 / math.sqrt(fan_in)\n",
        "                nn.init.uniform_(b, -bound, bound)\n",
        "\n",
        "    def forward(self, input: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n",
        "        x, channels = input\n",
        "        weight: Tensor = self.weight[channels]\n",
        "        output: Tensor = torch.bmm(x.transpose(0, 1), weight.transpose(1, 2)).transpose(0, 1)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias = self.bias[channels]\n",
        "            output = output + bias\n",
        "\n",
        "        if self.activation:\n",
        "            output = torch.tanh(output)\n",
        "\n",
        "        if self.residual:\n",
        "            if output.shape[2] == x.shape[2]:\n",
        "                output = output + x\n",
        "            elif output.shape[2] == x.shape[2] * 2:\n",
        "                output = output + torch.cat([x, x], dim=2)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Not implemented\")\n",
        "\n",
        "        return output, channels\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'num_channels={}, in_features={}, out_features={}, bias={}, activation={}, residual={}'.format(\n",
        "            self.num_channels, self.in_features, self.out_features, self.bias is not None, self.activation, self.residual\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8foxwRKw0rXP"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.4) Creating the BP Symmetry Functions***\n",
        "#@markdown We will define the BP and ANI symmetry functions to ensure that our \n",
        "#@markdown ML predictions are invariant to translations, rotations, and permutations. \n",
        "\n",
        "def pairwise_vector(coords: Tensor) -> Tensor:\n",
        "    num_batches, num_channels, _ = coords.size()\n",
        "    rij = coords[:, :, None] - coords[:, None]\n",
        "    mask = ~torch.eye(num_channels, dtype=torch.bool, device=coords.device) # remove self-interaction\n",
        "    rij = torch.masked_select(rij, mask.unsqueeze(2)).view(num_batches, num_channels, num_channels - 1, 3)\n",
        "    return rij\n",
        "\n",
        "\n",
        "def symmetry_function_g2(rij: Tensor, Rcr: float, EtaR: Tensor, ShfR: Tensor) -> Tensor:\n",
        "    dij = torch.norm(rij, dim=3)\n",
        "    fij = (torch.cos(dij / Rcr * math.pi) + 1) * 0.5\n",
        "    g2 = torch.sum(torch.exp(-EtaR.unsqueeze(dim=1) * (dij.unsqueeze(dim=-1) - ShfR.unsqueeze(dim=1))**2) * fij.unsqueeze(dim=-1), dim=2)\n",
        "    return g2\n",
        "\n",
        "\n",
        "def symmetry_function_g3(rij: Tensor, Rca: float, Zeta: Tensor, EtaA: Tensor) -> Tensor:\n",
        "    c = torch.combinations(torch.arange(rij.size(2)), r=2)\n",
        "    rij = rij[:, :, c]\n",
        "    r12 = rij[:, :, :, 0]\n",
        "    r13 = rij[:, :, :, 1]\n",
        "    r23 = r12 - r13\n",
        "    d12 = torch.norm(r12, dim=3)\n",
        "    d13 = torch.norm(r13, dim=3)\n",
        "    d23 = torch.norm(r23, dim=3)\n",
        "    f12 = (torch.cos(d12 / Rca * math.pi) + 1) * 0.5\n",
        "    f13 = (torch.cos(d13 / Rca * math.pi) + 1) * 0.5\n",
        "    f23 = (torch.cos(d23 / Rca * math.pi) + 1) * 0.5\n",
        "    cosine = torch.einsum('ijkl,ijkl->ijk', r12, r13) / (d12 * d13)\n",
        "\n",
        "    g3 = torch.sum(2**(1 - Zeta.unsqueeze(dim=1)) * (1 + cosine.unsqueeze(dim=-1))**Zeta.unsqueeze(dim=1) * torch.exp(-EtaA.unsqueeze(dim=1) * (d12**2 + d13**2 + d23**2).unsqueeze(dim=-1)) * (f12 * f13 * f23).unsqueeze(dim=-1), dim=2)\n",
        "    return g3\n",
        "\n",
        "\n",
        "def symmetry_function_g3ani(rij: Tensor, Rca: float, Zeta: Tensor, ShfZ: Tensor, EtaA: Tensor, ShfA: Tensor) -> Tensor:\n",
        "    c = torch.combinations(torch.arange(rij.size(2)), r=2)\n",
        "    rij = rij[:, :, c]\n",
        "    r12 = rij[:, :, :, 0]\n",
        "    r13 = rij[:, :, :, 1]\n",
        "    r23 = r12 - r13\n",
        "    d12 = torch.norm(r12, dim=3)\n",
        "    d13 = torch.norm(r13, dim=3)\n",
        "    f12 = (torch.cos(d12 / Rca * math.pi) + 1) * 0.5\n",
        "    f13 = (torch.cos(d13 / Rca * math.pi) + 1) * 0.5\n",
        "    cosine = torch.einsum('ijkl,ijkl->ijk', r12, r13) / (d12 * d13)\n",
        "    cosine = torch.cos(torch.acos(cosine).unsqueeze(dim=-1) - ShfA.unsqueeze(dim=1))\n",
        "\n",
        "    g3 = torch.sum(2**(1 - Zeta.unsqueeze(dim=1)) * (1 + cosine)**Zeta.unsqueeze(dim=1) * torch.exp(-EtaA.unsqueeze(dim=1) * (0.5 * (d12 + d13).unsqueeze(dim=-1) - ShfZ.unsqueeze(dim=1))**2) * (f12 * f13).unsqueeze(dim=-1), dim=2)\n",
        "    return g3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRXofpIJkKnd"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.5) Defining the BP Feature Class***\n",
        "#@markdown Now we will create the feature class, which takes input information and \n",
        "#@markdown uses functions to extract characteristics unique to the input called features.\n",
        "#@markdown Here we will be using the previously defined symmetry functions for feature\n",
        "#@markdown extraction.\n",
        "\n",
        "class Feature(nn.Module):\n",
        "    def __init__(self, Rcr: float, EtaR: Tensor, ShfR: Tensor, Rca: float, Zeta: Tensor, EtaA: Tensor) -> None:\n",
        "        super().__init__()\n",
        "        assert len(EtaR) == len(ShfR)\n",
        "        assert len(Zeta) == len(EtaA)\n",
        "        self.Rcr = Rcr\n",
        "        self.Rca = Rca\n",
        "        self.EtaR = torch.Tensor(EtaR)\n",
        "        self.ShfR = torch.Tensor(ShfR)\n",
        "        self.Zeta = torch.Tensor(Zeta)\n",
        "        self.EtaA = torch.Tensor(EtaA)\n",
        "\n",
        "    def forward(self, coords: Tensor, atom_types: Tensor) -> Tensor:\n",
        "        num_batches, num_channels, _ = coords.size()\n",
        "        rij = pairwise_vector(coords)\n",
        "        EtaR = self.EtaR[atom_types].to(device=coords.device)\n",
        "        ShfR = self.ShfR[atom_types].to(device=coords.device)\n",
        "        Zeta = self.Zeta[atom_types].to(device=coords.device)\n",
        "        EtaA = self.EtaA[atom_types].to(device=coords.device)\n",
        "        g2 = symmetry_function_g2(rij, self.Rcr, EtaR, ShfR)\n",
        "        g3 = symmetry_function_g3(rij, self.Rca, Zeta, EtaA)\n",
        "\n",
        "        return torch.concat((g2, g3), dim=2)\n",
        "\n",
        "    @property\n",
        "    def output_length(self) -> int:\n",
        "        return len(self.EtaR[0]) + len(self.EtaA[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st4TXB2yeW6v"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.6) Defining the ANI Feature Class***\n",
        "#@markdown We will create a similar feature class for ANI that uses the ANI \n",
        "#@markdown angle symmetry function. \n",
        "\n",
        "class FeatureANI(nn.Module):\n",
        "    def __init__(self, Rcr: float, EtaR: Tensor, ShfR: Tensor, Rca: float, Zeta: Tensor, ShfZ: Tensor, EtaA: Tensor, ShfA: Tensor) -> None:\n",
        "        super().__init__()\n",
        "        assert len(EtaR) == len(ShfR)\n",
        "        assert len(Zeta) == len(ShfZ) == len(EtaA) == len(ShfA)\n",
        "        self.Rcr = Rcr\n",
        "        self.Rca = Rca\n",
        "        self.EtaR = torch.Tensor(EtaR)\n",
        "        self.ShfR = torch.Tensor(ShfR)\n",
        "        self.Zeta = torch.Tensor(Zeta)\n",
        "        self.ShfZ = torch.Tensor(ShfZ)\n",
        "        self.EtaA = torch.Tensor(EtaA)\n",
        "        self.ShfA = torch.Tensor(ShfA)\n",
        "\n",
        "    def forward(self, coords: Tensor, atom_types: Tensor) -> Tensor:\n",
        "        num_batches, num_channels, _ = coords.size()\n",
        "        rij = pairwise_vector(coords)\n",
        "        EtaR = self.EtaR[atom_types].to(device=coords.device)\n",
        "        ShfR = self.ShfR[atom_types].to(device=coords.device)\n",
        "        Zeta = self.Zeta[atom_types].to(device=coords.device)\n",
        "        ShfZ = self.ShfZ[atom_types].to(device=coords.device)\n",
        "        EtaA = self.EtaA[atom_types].to(device=coords.device)\n",
        "        ShfA = self.ShfA[atom_types].to(device=coords.device)\n",
        "        g2 = symmetry_function_g2(rij, self.Rcr, EtaR, ShfR)\n",
        "        g3 = symmetry_function_g3ani(rij, self.Rca, Zeta, ShfZ, EtaA, ShfA)\n",
        "\n",
        "        return torch.concat((g2, g3), dim=2)\n",
        "\n",
        "    @property\n",
        "    def output_length(self) -> int:\n",
        "        return len(self.EtaR[0]) + len(self.EtaA[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02e3WPabvjIl"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.7) Creating the Fitting Neural Network***\n",
        "#@markdown Now we create the Fitting class, which will use the dense neural \n",
        "#@markdown network for fitting.\n",
        "\n",
        "class Fitting(nn.Module):\n",
        "    def __init__(self, n_types: int, in_features: int, neuron: Sequence[int] = [240, 240, 240]) -> None:\n",
        "        super().__init__()\n",
        "        layers = [Dense(n_types, in_features, neuron[0], activation=True)]\n",
        "        for i in range(len(neuron)-1):\n",
        "            layers.append(Dense(n_types, neuron[i], neuron[i+1], activation=True, residual=True))\n",
        "        layers.append(Dense(n_types, neuron[-1], 1))\n",
        "        self.fitting_net = Sequential(*layers)\n",
        "\n",
        "    def forward(self, input : Tuple[Tensor, Tensor]) -> Tensor:\n",
        "        output, _ = self.fitting_net(input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw-8zbn_wPz5"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.8) The Behler-Parrinello Fitting Neural Network (BP-FNN)***\n",
        "#@markdown We create the BPNN class that can be used for feature extraction with the \n",
        "#@markdown symmetry functions.\n",
        "\n",
        "class BPNN(pl.LightningModule):\n",
        "    def __init__(self, descriptor: nn.Module, fitting_net: nn.Module, learning_rate=5e-4) -> None:\n",
        "        super().__init__()\n",
        "        self.descriptor = descriptor\n",
        "        self.fitting_net = fitting_net\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, coords: torch.Tensor, atom_types: torch.Tensor):\n",
        "        coords.requires_grad_()\n",
        "        descriptors = self.descriptor(coords, atom_types)\n",
        "        atomic_energies = self.fitting_net((descriptors, atom_types))\n",
        "        energy = torch.unbind(torch.sum(atomic_energies, dim=1))\n",
        "        gradient, = torch.autograd.grad(energy, [coords], create_graph=True)\n",
        "        return torch.hstack(energy), gradient\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        qm_coord, atom_types, energy, gradient = batch\n",
        "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
        "        ene_loss = F.mse_loss(ene_pred, energy)\n",
        "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
        "\n",
        "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
        "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
        "        w_ene = 1\n",
        "        w_grad = 1 + 99 * (lr / start_lr)\n",
        "\n",
        "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('l2_trn', torch.sqrt(loss))\n",
        "        self.log('l2_e_trn', torch.sqrt(ene_loss))\n",
        "        self.log('l2_f_trn', torch.sqrt(grad_loss))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        torch.set_grad_enabled(True)\n",
        "        qm_coord, atom_types, energy, gradient = batch\n",
        "        ene_pred, grad_pred = self(qm_coord, atom_types[0])\n",
        "        ene_loss = F.mse_loss(ene_pred, energy)\n",
        "        grad_loss = F.mse_loss(grad_pred, gradient)\n",
        "\n",
        "        lr = self.optimizers().optimizer.param_groups[0]['lr']\n",
        "        start_lr = self.optimizers().optimizer.param_groups[0]['initial_lr']\n",
        "        w_ene = 1\n",
        "        w_grad = 1 + 99 * (lr / start_lr)\n",
        "\n",
        "        loss = w_ene / (w_ene + w_grad) * ene_loss + w_grad / (w_ene + w_grad) * grad_loss\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('l2_tst', torch.sqrt(loss))\n",
        "        self.log('l2_e_tst', torch.sqrt(ene_loss))\n",
        "        self.log('l2_f_tst', torch.sqrt(grad_loss))\n",
        "        self.log('lr', lr)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95),\n",
        "                     'interval': 'epoch',\n",
        "                     'frequency': 10,\n",
        "                    }\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNbkW3uUOSOq"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.9) Importing Data and Seeding***\n",
        "#@markdown We will now import our semiempirical and DFT calculation data.\n",
        "#@markdown We will then use seeding to generate random but reproducible \n",
        "#@markdown training. \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "qm_coord = torch.from_numpy(np.array(np.load(\"qm_coord.npy\"), dtype=\"float32\")).cuda()\n",
        "atom_types = np.loadtxt(\"qm_elem.txt\", dtype=int)\n",
        "elems = np.unique(atom_types).tolist()\n",
        "atom_types = torch.from_numpy(np.array([elems.index(i) for i in atom_types])).cuda()\n",
        "atom_types = atom_types.repeat(len(qm_coord), 1)\n",
        "\n",
        "energy = torch.from_numpy(np.array((np.load(\"energy.npy\") - np.load(\"energy_sqm.npy\")) * 27.2114 * 23.061, dtype=\"float32\")).cuda()\n",
        "energy = energy - energy.mean()\n",
        "qm_gradient = torch.from_numpy(np.array((np.load(\"qm_grad.npy\") - np.load(\"qm_grad_sqm.npy\")) * 27.2114 * 23.061 / 0.529177249, dtype=\"float32\")).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fc0zx5YOn-y"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(2)\n",
        "dataset = TensorDataset(qm_coord, atom_types, energy, qm_gradient)\n",
        "train, val = random_split(dataset, [2016, 84])\n",
        "train_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FYCh2Le7jRr"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.10) Setting ANI Parameters***\n",
        "#@markdown Here we set the parameters for the ANI model\n",
        "#@markdown and begin training our BP-FNN.\n",
        "\n",
        "%%time\n",
        "pl.seed_everything(2)\n",
        "\n",
        "ani = True\n",
        "\n",
        "if ani:\n",
        "  # From TorchANI\n",
        "  Rcr = 5.2000e+00\n",
        "  Rca = 3.5000e+00\n",
        "  EtaR = [1.6000000e+01]\n",
        "  ShfR = [9.0000000e-01,1.1687500e+00,1.4375000e+00,1.7062500e+00,1.9750000e+00,2.2437500e+00,2.5125000e+00,2.7812500e+00,3.0500000e+00,3.3187500e+00,3.5875000e+00,3.8562500e+00,4.1250000e+00,4.3937500e+00,4.6625000e+00,4.9312500e+00]\n",
        "  Zeta = [3.2000000e+01]\n",
        "  ShfZ = [1.9634954e-01,5.8904862e-01,9.8174770e-01,1.3744468e+00,1.7671459e+00,2.1598449e+00,2.5525440e+00,2.9452431e+00]\n",
        "  EtaA = [8.0000000e+00]\n",
        "  ShfA = [9.0000000e-01,1.5500000e+00,2.2000000e+00,2.8500000e+00]\n",
        "  EtaR, ShfR = np.array(np.meshgrid(EtaR, ShfR)).reshape(2, -1)\n",
        "  Zeta, ShfZ, EtaA, ShfA = np.array(np.meshgrid(Zeta, ShfZ, EtaA, ShfA)).reshape(4, -1)\n",
        "  EtaR = np.repeat([EtaR], 3, axis=0)\n",
        "  ShfR = np.repeat([ShfR], 3, axis=0)\n",
        "  Zeta = np.repeat([Zeta], 3, axis=0)\n",
        "  ShfZ = np.repeat([ShfZ], 3, axis=0)\n",
        "  EtaA = np.repeat([EtaA], 3, axis=0)\n",
        "  ShfA = np.repeat([ShfA], 3, axis=0)\n",
        "  descriptor = FeatureANI(Rcr, EtaR, ShfR, Rca, Zeta, ShfZ, EtaA, ShfA)\n",
        "else:\n",
        "  Rcr = 6.0\n",
        "  Rca = 6.0\n",
        "  ShfR = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]] # H, C, O\n",
        "  EtaR = [[0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79]] # H, C, O\n",
        "  Zeta = [[1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32], [1, 2, 4, 8, 16, 32]] # H, C, O\n",
        "  EtaA = [[0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79], [0.0, 0.04, 0.14, 0.32, 0.71, 1.79]] # H, C, O\n",
        "  descriptor = Feature(Rcr, EtaR, ShfR, Rca, Zeta, EtaA)\n",
        "\n",
        "fitting_net = Fitting(3, descriptor.output_length, neuron=[240, 240, 240])\n",
        "model = BPNN(descriptor, fitting_net, learning_rate=5e-4)\n",
        "csv_logger = pl_loggers.CSVLogger('logs_csv/')\n",
        "trainer = pl.Trainer(max_epochs=500, logger=csv_logger, log_every_n_steps=20, accelerator='gpu')\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDsRamgOkdXY"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.11) Saving model to PyTorch file***\n",
        "#@markdown The following files are saved:\n",
        "\n",
        "#@markdown **1) model.pt**\n",
        "#@markdown - torch.save saves tensors to model.pt\n",
        "\n",
        "#@markdown **2) model_script.pt**\n",
        "#@markdown - torch.jit.save attempts to preserve the behavior of some operators across PyTorch versions.\n",
        "\n",
        "#@markdown Previously saved models can be loaded with:\n",
        "#@markdown - model.load_state_dict(torch.load(' **1)** '))\n",
        "#@markdown - torch.jit.load(' **2)** ')\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pt')\n",
        "torch.jit.save(model.to_torchscript(), \"model_script.pt\")\n",
        "\n",
        "# To load models:\n",
        "# model.load_state_dict(torch.load('model_diff.pt'))\n",
        "# model = torch.jit.load('model_diff_script.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRMmWy9TPeJD"
      },
      "outputs": [],
      "source": [
        "ene_pred, grad_pred = model(qm_coord.cpu(), atom_types[0].cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlF1V44dPibM"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.12) Plotting RMSD for $\\Delta$MLP Energy and Forces***\n",
        "#@markdown RMSD for predicted and reference energy and forces are calculated and\n",
        "#@markdown displayed below. \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "e1 = energy.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
        "e2 = ene_pred.cpu().detach().numpy() + np.load(\"energy_sqm.npy\") * 27.2114 * 23.061\n",
        "ax[0].plot(e1, e2, linestyle='none', marker='.', )\n",
        "ax[0].plot([np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], [np.max(np.concatenate((e1,e2))), -np.max(np.concatenate((e1,e2)))], color=\"k\", linewidth=1.5)\n",
        "ax[0].set_xlabel(\"Reference Energy (kcal/mol)\", size=14)\n",
        "ax[0].set_ylabel(\"Predicted Energy (kcal/mol)\", size=14)\n",
        "ax[0].annotate('RMSD: %.3f' % np.sqrt(np.mean((e1 - e2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
        "\n",
        "f1 = -qm_gradient.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
        "f2 = -grad_pred.cpu().detach().numpy().reshape(-1) - np.load(\"qm_grad_sqm.npy\").reshape(-1) * 27.2114 * 23.061 / 0.529177249\n",
        "\n",
        "ax[1].plot(f1, f2, linestyle='none', marker='.', )\n",
        "plt.plot([-np.abs(np.max(np.concatenate((f1,f2)))), np.max(np.concatenate((f1,f2)))], [-np.max(np.concatenate((f1,f2))), np.max(np.concatenate((f1,f2)))], color=\"k\", linewidth=1.5)\n",
        "ax[1].set_xlabel(\"Reference Force (kcal/mol/A)\", size=14)\n",
        "ax[1].set_ylabel(\"Predicted Force (kcal/mol/A)\", size=14)\n",
        "ax[1].annotate('RMSD: %.3f' % np.sqrt(np.mean((f1 - f2)**2)), xy=(0.05, 0.95), xycoords='axes fraction', size=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rmsd.png', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLr5VkQCPoBO"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.13) The Model Weights and Biases Dictionary***\n",
        "#@markdown This cell prints the size of the weights and biases\n",
        "#@markdown used in the trained model for reference. \n",
        "\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzYcSyS4PrE2"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.14) Plotting Validation Error for $\\Delta$MLP***\n",
        "#@markdown Validation error for the $\\Delta$MLP is shown below. \n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('logs_csv/lightning_logs/version_10/metrics.csv')\n",
        "# data = pd.read_csv('/content/drive/MyDrive/f10_e500/logs_csv/lightning_logs/version_2/metrics.csv')\n",
        "fig, ax = plt.subplots(2, figsize=(6,10))\n",
        "x = data['epoch'][~data['l2_e_tst'].isnull()]\n",
        "y = data['l2_e_tst'][~data['l2_e_tst'].isnull()]\n",
        "y2 = data['l2_f_tst'][~data['l2_f_tst'].isnull()]\n",
        "ax[0].semilogy(x, y)\n",
        "ax[0].set_ylabel('Energy Validation Error (kcal/mol)')\n",
        "ax[1].semilogy(x, y2, label='Force Validation Error (kcal/mol')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Force Validation Error (kcal/mol/A)')\n",
        "fig.savefig('loss.png', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCew0e4YnCaG"
      },
      "outputs": [],
      "source": [
        "#@title ***(5.15) Plotting Training and Validation Loss for $\\Delta$MLP***\n",
        "#@markdown The loss at each step of the training process is displayed below. \n",
        "\n",
        "data = pd.read_csv('logs_csv/lightning_logs/version_10/metrics.csv')\n",
        "# data = pd.read_csv('/content/drive/MyDrive/f10_e500/logs_csv/lightning_logs/version_2/metrics.csv')\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "x = data['epoch'][~data['epoch'].isnull()]\n",
        "y = data['train_loss'][~data['train_loss'].isnull()]\n",
        "print(len(y))\n",
        "plt.semilogy(y, label='Train Loss')\n",
        "y = data['val_loss'][~data['val_loss'].isnull()]\n",
        "print(len(y))\n",
        "plt.semilogy(y, label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
